# -*- coding: utf-8 -*-
"""Regresión logística - descenso de gradiente.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EaKESeLq0D3ZlfEH8NOR3jvGwiB4NBU2

### Regresión Logística y Función Sigmoide

La regresión logística es un método estadístico para predecir clases binarias. El resultado o variable objetivo es de naturaleza dicotómica. La regresión logística predice la probabilidad de que la variable objetivo pertenezca a una de las dos clases.

La función sigmoide, también conocida como función logística, se utiliza para convertir cualquier valor real en otro valor entre 0 y 1, y se define como:

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

Donde:
- $ \sigma(z) $ es el valor de salida de la función sigmoide.
- $ z $ es la entrada a la función, que puede ser cualquier valor real.
"""

import numpy as np

# Función sigmoide, que es nuestra función de hipótesis para la regresión logística
def sigmoid(z):
    return(1/1 + np.exp(-z))

"""## Descenso de Gradiente Estocástico

### Cálculo del gradiente
Las derivadas parciales de J son:
\begin{align*}
    \frac{\partial J}{\partial b}(b,w) = a-y \qquad  \frac{\partial J}{\partial w_i}(b,w) = (a-y)x_i
\end{align*}
"""

def compute_gradients(x, y, w, b):
    # Calcula la entrada lineal z y la activación a

    # Término de bias
    z = b
    for i in range(len(w)):
      z += w[i] * x[i]

    a = sigmoid(z)

    # Calcula las derivadas
    db = a-y #resulta en un escalar

    dw = [(a-y)*x_i for x_i in x] #resulta un vector o lista de dimensión p

    return db, dw

"""### Descenso de gradiente

En primer lugar se inicializan los pesos b y w de manera aleatoria. Después se repite la iteración:
  
  1. Se introduce una observación por la entrada: $x$

  2. Se obtienen el valor de salida: $a$

  3. Con $a$ y $x$ podemos calcular el valor numérico del gradiente evaluado en b y w


  \begin{align*}
      & \frac{\partial J}{\partial b} = a-y  \qquad
      & \frac{\partial J}{\partial w} = (a-y)x
  \end{align*}

  4. Actualizamos los pesos de acuerdo a las ecuaciones (descenso de gradiente)


  \begin{align*}
  b := b - \eta \frac{\partial J}{\partial b} \big(b,w \big) \qquad
  w := w - \eta \frac{\partial J}{\partial w} \big(b,w \big)
  \end{align*}
"""

# Aquí escribimos los pasos de función iterativa:
def sgd_single_observation(x,y,w,b,eta):

    # Calculamos los gradientes con la función implementada anteriormente
    db, dw = compute_gradients(x, y, w, b)

    # Actualizar el término de bias b utilizando su derivada
    b = b - eta * db

    # Actualizar cada peso w_i utilizando su correspondiente derivada
    for index in range(len(w)):
      w[index] = w[index] - eta * dw[index]
    return b, w

"""### Función que implementa la regresión logística con descenso de gradiente

Haremos una inicialización de los pesos y ejecutaremos la iteraciones definidas.


La inicialización se realizará de forma aleatoria y con valores moderados, fundamentalmente por dos razones:


1. **Evitar la saturación temprana:** Previene la inactividad del aprendizaje por la saturación de la función sigmoide.
2. **Prevenir sobreajuste inicial:** Ayuda a evitar un ajuste excesivo temprano.

Esta práctica facilita un mejor aprendizaje y es un buen punto de partida, aunque la estrategia óptima puede variar según el caso específico.
"""

def logistic_regression_sgd(X, y, eta , iterations):
    # Número de características (p)
    p = X.shape[1]

    # Numero de ejemplos (N)

    N = X.shape[0]

    # Inicializar los pesos (w) y el término de sesgo (b) con valores aleatorios de N(0,1)
    w = np.random.randn(p)
    b = np.random.randn()

    for i in range(iterations):
        # Seleccionar aleatoriamente una observación de los datos de entrenamiento
        random_index = np.random.randint(0, N)
        x_i=X[random_index]
        y_i=y[random_index]

        # Actualizar b y w para la observación seleccionada usando SGD
        b, w = sgd_single_observation(x=x_i,y=y_i,b=b,w=w,eta=eta)



    return b, w

"""### Datos de Iris"""

from sklearn.datasets import load_iris
import matplotlib.pyplot as plt


# Cargamos el conjunto de datos Iris
iris = load_iris()
X = iris.data[:, :1]  # Tomamos solo la primera característica.
y = iris.target

# Creamos un filtro para quedarnos solo con dos clases (clasificación binaria)
filter = (y == 0) | (y == 1)
X = X[filter]
y = y[filter]

# Híper-parámetros de entrenamiento
eta = 0.2  # Tasa de aprendizaje
iterations = 1000  # Número de iteraciones

# Entrenamiento del modelo
b, w = logistic_regression_sgd(X=X, y=y, eta = eta, iterations = iterations)

# Mostrar los pesos con 2 decimales únicamente
print(f'Bias final {b:.2f}')
print(f'w final {w[0]:.2f}')

print(f'ecuación de frontera: x = {-b/w[0]:.2f}')

"""La ecuación de frontera venía dada por -b/w. Utilizando las librerías nos daba un valor de aproximadamente x=5.4

Con estos parámetros de entrenamiento encontramos mucha variabilidad en la solución.
"""

-b/w

"""Para entender mejor el proceso de entrenamiento, vamos a monitorizarlo mediante la evolución de la función de pérdida:

### Función de coste para **una observación**

De dimensión 1:
\begin{align*}
  & a \equiv\sigma(z) = \sigma(b+wx) \\ \\
\end{align*}

De dimensión p:
\begin{align*}
  & a \equiv\sigma(z) = \sigma(b + w_1 x_1 + .... + w_p x_p) \\ \\
\end{align*}


\begin{align*}
        & J = - \left[ y \log a + (1 - y) \log (1 - a) \right]
\end{align*}
"""

def cost_function_single_observation(x, y, w, b):

    # Calculamos la entrada lineal z
    # Término de bias
    z = b

    #Resto de términos
    for i in range(len(w)):  # Suma ponderada de características y pesos
        z += w[i] * x[i]

    # Calcula la activación a
    a = sigmoid(z)

    # Calcula el costo J para una única observación
    J = -(y * np.log(a) + (1-y) * np.log(1-a))

    return J

"""Y se la añadimos a nuestra función general"""

def logistic_regression_sgd_loss(X, y, eta, iterations):
    # Número de características (p)
    p = X.shape[1]

    # Numero de ejemplos (N)

    N = X.shape[0]

    # Inicializar los pesos (w) y el término de sesgo (b) con valores aleatorios de N(0,1)
    w = np.random.randn(p)
    b = np.random.randn()

    losses=[]
    for i in range(iterations):
        # Seleccionar aleatoriamente una observación de los datos de entrenamiento
        random_index = np.random.randint(0, N)
        x_i=X[random_index]
        y_i=y[random_index]

        # Actualizar b y w para la observación seleccionada usando SGD
        b, w = sgd_single_observation(x=x_i,y=y_i,b=b,w=w,eta=eta)

        #actualizamos la lista de perdida
        loss = cost_function_single_observation(x=x_i, y=y_i, w=w, b=b)
        losses.append(loss)
    return b, w, losses

eta = 0.01  # Tasa de aprendizaje
iterations = 1000  # Número de iteraciones

# Entrenamiento del modelo
b, w, loss = logistic_regression_sgd_loss(X=X, y=y, eta=eta, iterations=iterations)

# Representamos la evolución de la pérdida
plt.plot(loss[10:], color='blue')
plt.title('Evolución de la pérdida durante el entrenamiento')
plt.xlabel('Iteraciones')
plt.ylabel('Costo')
plt.show()

"""Lo representamos para diferentes valores de $\eta$"""

import numpy as np
import matplotlib.pyplot as plt

# Definir diferentes tasas de aprendizaje para experimentar con ellas (0.0001, 0.001, 0.01, 0.1)
etas = [0.0001, 0.001, 0.01, 0.1]

# Establecer el número total de iteraciones para el entrenamiento(10000)
iterations = 30000

# Iterar sobre cada tasa de aprendizaje
for eta in etas:
    # Establecer una semilla (1) para la reproducibilidad de los resultados
    np.random.seed(1)

    # Ejecutar la regresión logística estocástica (SGD) con la tasa de aprendizaje actual
    b, w, loss = logistic_regression_sgd_loss(X=X, y=y, eta=eta, iterations=iterations)

    # Representar la evolución de la pérdida después de las primeras 100 iteraciones
    # para evitar grandes valores iniciales que distorsionan la escala del gráfico
    plt.plot(loss[100:], label = '$\eta$ = ' + str(eta))

# Configurar el título y las etiquetas del gráfico
plt.title('Evolución de la pérdida durante el entrenamiento')
plt.xlabel('Iteraciones')
plt.ylabel('Costo')

# Mostrar la leyenda
plt.legend()

# Mostrar el gráfico
plt.show()

def calculate_accuracy(X_test, y_test, b, w):
  #Inicializamos una lista para almacenar las predicciones
  predictions = []

  N_test = X_test.shape[0]
  p = X_test.shape[1]
  #Iteramos a través de las caracteristicas
  for i in range(N_test):
    #Calculamos z
    z=b
    for j in range(p):
      z += X_test[i,j] * w[j]

    prediction = sigmoid(z)

    predictions.append(prediction >= 0.5)


  #Calculamos
  predictions = np.array(predictions)

  #Determinamos si la predicción es 0, 1
  return (predictions == y_test).mean() * 100

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

eta = 0.1
iterations = 10000

b, w, costs = logistic_regression_sgd_loss(X=X_train,y=y_train,eta=eta,iterations=iterations)

accuracy = calculate_accuracy(X_test=X_test, y_test=y_test, b=b, w=w)

print(f'Presición con eta {eta}: {accuracy:.2f}%')

"""## Experimento con p=2"""

rom sklearn.datasets import load_iris
import matplotlib.pyplot as plt


# Cargamos el conjunto de datos Iris
iris = load_iris()
X = iris.data[:, :2]  # Tomamos solo la primera característica.
y = iris.target

# Creamos un filtro para quedarnos solo con dos clases (clasificación binaria)
filter = (y == 0) | (y == 1)
X = X[filter]
y = y[filter]

eta = 0.1
iterations = 10000

#Separamos train-test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)

# Entrenqamos el modelo para p=2
b, w, costs = logistic_regression_sgd_loss(X=X_train,y=y_train,eta=eta,iterations=iterations)

# Evaluamos la presición
accuracy = calculate_accuracy(X_test=X_test, y_test=y_test, b=b, w=w)

print(f'Presición con eta {eta}: {accuracy:.2f}%')

