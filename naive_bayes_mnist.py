# -*- coding: utf-8 -*-
"""Naive Bayes Mnist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LrpmUvD5CDGPKzI0ZRsp6BdhjxjqtXHj

Procedimiento del Análisis Discriminante

1. Establecer un modelo para $p(x|y=c)$ para cada clase $c$.
2. Estimar los parámetros $\theta_c$ de cada modelo mediante máxima verosimilitud.
3. Calcular la probabilidad a posteriori $p(y=c|x)$ usando el Teorema de Bayes.
4. Asignar las instancias que deseamos predecir a la clase que maximice la probabilidad posterior.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Cargar el dataset MNIST
X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')
y = y.astype(int)  # Convertir las etiquetas a enteros

"""### Mostramos por pantalla un dato de ejemplo"""

X.shape

y.shape

y == 1

# Seleccionar la primera imagen de las filtradas y cambiar su forma a 28x28 para la visualización
image = X[0].reshape(28, 28)  # Cambiar la forma para visualización

# Visualizar la imagen
plt.imshow(image, cmap='gray')  # Usar un mapa de colores en escala de grises
plt.title('Digit: 1')
plt.axis('off')  # Esconder los ejes para una mejor visualización
plt.show()

"""### Tomamos todas las imágenes de una clase"""

# Vamos a extraer las imágenes de la clase 1:

# Sacamos los índices del vector y que corresponden a la clase 1
indices_clase1 = y==1

# Seleccionamos X de acuerdo a esos índices
X_1 = X[indices_clase1]

# Obtenemos la dimensión de la matriz seccionada:
X_1.shape

#Tenemos 7877 ejemplos de 784 dimensiones cada uno.

"""## Binarizamos los datos"""

X[0]

# Binarizar los datos: píxeles mayores que 127.5 se convierten en 1, de lo contrario en 0
X_binarized = np.where(X > 127.5, 1, 0)

# Dividir el dataset en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X_binarized, y, test_size=0.2, random_state=42)

# Otra forma de binarizar los datos

X_binarized_ = X > 127.5 #Usamos la mascara
X_binarized_[0].astype(int)  #pasamos los booleanos a ceros y unosb

"""# 1 - Escogemos un modelo para $p(x|y=c)$ para cada clase $c$: Naive Bayes binarizado.

Ahora vamos a implementar el algoritmo Naive Bayes. Naive Bayes se basa en el teorema de Bayes, con la "ingenua" suposición de independencia entre las características. La fórmula para el teorema de Bayes es:

# 2 - Estimar los parámetros $\theta_c$ de cada modelo mediante máxima verosimilitud.

\begin{align*}
        \hat{\theta}_{ic} &= \frac{\sum_{n=1}^N x_{ni} \ \mathbb{I}(y_n = c)}{\sum_{n=1}^N \mathbb{I}(y_n = c)}  = \frac{N_{ic}}{N_c} \\\\
        \hat{\pi_c} &= \frac{\sum_{n=1}^N \mathbb{I}(y_n = c)}{N} = \frac{N_c}{N}
\end{align*}

Función para extraer las imágenes de una clase
"""

def extract_class_images(X, y, class_label):
    """
    Extrae todas las imágenes que pertenecen a una clase específica.

    Parámetros:
        X (numpy.ndarray): Las imágenes del conjunto de datos (ej., MNIST).
        y (numpy.array): Las etiquetas correspondientes de las imágenes.
        class_label (int): La etiqueta de la clase para extraer las imágenes.

    Devuelve:
        numpy.ndarray: Un array de las imágenes que pertenecen a la clase específica.
    """
    extracted_images = X[y == class_label]
    # Asegurarse de que las imágenes extraídas corresponden a la clase correcta
    assert (y[y == class_label] == class_label).all(), "No todas las imágenes extraídas corresponden a la clase solicitada."
    return extracted_images


# Crear un conjunto de datos de prueba pequeño y sencillo
X_check = np.array([[0, 1], [1, 1], [0, 0]])  # Supongamos que son 'imágenes' con dos píxeles
y_check = np.array([0, 1, 0])  # Etiquetas correspondientes
class_label_check = 0  # Clase que queremos extraer

extract_class_images(X=X_check, y=y_check, class_label = class_label_check)

"""Debe dar

```
array([[0, 1],
       [0, 0]])
```

### 2.1 - Función para estimar las probabilidades a priori de cada clase:

\begin{align*}
    \hat{\pi_c} &= \frac{\sum_{n=1}^N \mathbb{I}(y_n = c)}{N} = \frac{N_c}{N}
\end{align*}
"""

# Ejemplo de etiquetas de clase, donde cada número representa una categoría diferente
y_check = np.array([0, 1, 2, 1, 1, 0, 2, 0, 0, 1, 2, 0, 1, 2, 1])

# Obtener clases únicas y su recuento correspondiente
clases, cantidad = np.unique(y_check, return_counts=True)

# Mostrar resultados
print("Clases:", clases)  # Clases únicas
print("Cantidad por clase:", cantidad)  # Cantidad de elementos por clase

0.2+0.7+0.1 #problema de numpy

np.isclose(0.2 + 0.7 +0.1, 1)

import numpy as np

def calculate_class_priors(y):
    """
    Calcula las probabilidades a priori de las clases basadas en las etiquetas.

    Parámetros:
        y (numpy.array): Las etiquetas del conjunto de datos.

    Retorna:
        numpy.ndarray: Un array que contiene la probabilidad a priori para cada clase.
    """
    # Contar la aparición de cada clase en el conjunto de etiquetas 'y'
    classes, counts = np.unique(y, return_counts = True)


    # Calcular las probabilidades a priori para cada clase.
    # La probabilidad a priori de una clase es el número de apariciones de esa clase dividido por el número total de muestras.
    priors = counts / float(len(y))

    # Asegurarse de que las probabilidades a priori suman 1 (esto es una buena práctica para verificar la validez de las probabilidades).
    assert np.isclose(priors.sum(), 1.0), "Las probabilidades a priori no suman 1."

    # Retornar el array de las probabilidades a priori calculadas.
    return priors

# Crear un conjunto de datos de prueba pequeño y sencillo para etiquetas
y_check = np.array([0, 1, 0, 1, 0])  # Ejemplo de etiquetas

# Llamar a la función y almacenar las probabilidades a priori resultantes
priors_check = calculate_class_priors(y=y_check)

# Imprimir las probabilidades a priori calculadas para verificar
print("Probabilidades a priori calculadas:", priors_check)

# Verificación simple de las probabilidades a priori calculadas (sin usar una función de test independiente)
# Debería haber dos clases con probabilidades 0.6 y 0.4 respectivamente para este conjunto de datos de prueba
assert np.isclose(priors_check[0], 0.6), "Probabilidad a priori incorrecta para la clase 0."
assert np.isclose(priors_check[1], 0.4), "Probabilidad a priori incorrecta para la clase 1."
print("La verificación de las probabilidades a priori pasó correctamente.")

"""### 2.2 - Función para estimar los parámetros

Utilizaremos el suavizado de Laplace:

\begin{align*}
        \hat{\theta}_{ic}^{\text{(Laplace)}}  = \frac{N_{ic}+1}{N_c+2}
\end{align*}
"""

def get_theta_estimate(X_c):
    """
    Calcula las estimaciones de parámetros theta para una clase dada, aplicando suavizado de Laplace.

    Parámetros:
        X_c (numpy.ndarray): Las imágenes del conjunto de datos que pertenecen a una clase específica.

    Retorna:
        numpy.ndarray: Un array que contiene la probabilidad de que cada píxel sea blanco (1) en imágenes
                       de la clase específica, con suavizado de Laplace aplicado.
    """
    # X_c es una matriz donde cada fila contiene los píxeles de una image (784)
    # y tendrá tantas filas como imagen de la clase en cuestión.

    #Calculamos N_ic + 1 con matrices (intentarlo mejor sin bucles)
    N_ic = X_c.sum(axis=0)

    #Calculamos N_c
    N_c = X_c.shape[0]

    theta_ic_estimates = (N_ic + 1) / (N_c + 2)

    # Asegurarse de que todas las probabilidades estén entre 0 y 1
    assert np.all((theta_ic_estimates >= 0) & (theta_ic_estimates <= 1)), "Las probabilidades estimadas deben estar entre 0 y 1."

    # Retornar el array de las estimaciones de theta para la clase.
    return theta_ic_estimates

# Crear un conjunto de datos de prueba pequeño y sencillo
X_check_class = np.array([[0, 1], [1, 1], [1, 0]])  # Supongamos que estas son todas las imágenes de una clase específica

# Llamar a la función y almacenar las estimaciones de theta resultantes
theta_estimates_check = get_theta_estimate(X_check_class)

# Imprimir las estimaciones de theta para verificar
print("Estimaciones de Theta calculadas:", theta_estimates_check)

# Verificación simple de las estimaciones de theta (sin usar una función de prueba independiente)
# Asegurar que todas las estimaciones estén entre 0 y 1
assert np.all((theta_estimates_check >= 0) & (theta_estimates_check <= 1)), "Las estimaciones de Theta deben estar entre 0 y 1."
print("La verificación de las estimaciones de Theta pasó correctamente.")

# X_c es una matriz donde cada fila contiene los píxeles de una image (784)
# y tendrá tantas filas como imagen de la clase en cuestión.
X_c_check = extract_class_images(X=X_binarized, y=y, class_label=1)

X_c_check.shape

# Si sumamos todos los elementos de cada columna, tendremos los N_ic de la clase c
X_c_check.sum(axis=0).shape

np.unique(y)

def fit_naive_bayes(X, y):
    # Obtener el número de muestras (imágenes) y de características (píxeles) del conjunto de datos.
    n_samples, p = X.shape

    # Identificar las clases únicas en el conjunto de etiquetas 'y'.
    # En MNIST, estas clases corresponden a los dígitos del 0 al 9.
    classes = np.unique(y)
    n_classes = len(classes)  # Número de clases únicas

    # Inicializar las probabilidades a priori para cada clase.
    priors_estimates = calculate_class_priors(y)

    # Inicializar el array de parámetros thetas_estimates con las dimensiones correctas
    thetas_estimates = np.zeros((p, n_classes))

    # Calcular las probabilidades para cada clase
    for i, c in enumerate(classes):
        # Extraer todas las imágenes que pertenecen a la clase 'c'.
        X_c = extract_class_images(X=X, y=y, class_label=c)

        # Calculamos los parámetros de cada clase
        class_thetas = get_theta_estimate(X_c)
        thetas_estimates[:, i] = class_thetas

    # La función devuelve las probabilidades a priori, las probabilidades de verosimilitud para cada clase y píxel,
    # y la lista de clases.
    return priors_estimates, thetas_estimates

"""# 3 y 4 - Predecimos las clases

\begin{align*}
        & \hat{y} =
        \arg \max_c \left(
        \hat{\pi}_c\prod_{i=1}^{784} \hat{\theta}_{ic}^{x_i}(1- \hat{\theta}_{ic})^{1-x_i}
        \right)
\end{align*}

Para calcular $\hat{y}$ utilizamos logaritmos por eficiencia computacional y claridad de código:




\begin{align*}
  \log \hat{y} &= \log \left(\arg \max_c \left(
  \hat{\pi}_c\prod_{i=1}^{784} \hat{\theta}_{ic}^{x_i}(1- \hat{\theta}_{ic})^{1-x_i}
  \right)\right)\\
  &= \arg \max_c \log \left(
  \hat{\pi}_c\prod_{i=1}^{784} \hat{\theta}_{ic}^{x_i}(1- \hat{\theta}_{ic})^{1-x_i}
  \right)
\end{align*}

Usando las propiedades de los logaritmos:

\begin{align*}
  \log \hat{y} &= \arg \max_c \left(
  \log \hat{\pi}_c + \sum_{i=1}^{784} \left(x_i \log \hat{\theta}_{ic} + (1-x_i) \log (1- \hat{\theta}_{ic})\right)
  \right)
\end{align*}
"""

import numpy as np

def predict_naive_bayes(X, priors_estimates, thetas_estimates):
    """
    Predice las clases de múltiples muestras utilizando el modelo Naive Bayes.

    Parámetros:
    - X (numpy.array): Las muestras de entrada (n_samples, 784).
    - priors_estimates (numpy.array): Las probabilidades a priori de las clases.
    - thetas_estimates (numpy.array): Las probabilidades condicionales de los píxeles dado la clase.

    Retorna:
    - numpy.array: Las clases predichas para cada muestra.
    """
    # Paso 1: Calcular el número de muestras y clases
    n_samples = X.shape[0]
    n_classes = len(priors_estimates)

    # Paso 2: Inicializar un array para almacenar las probabilidades logarítmicas de cada clase para cada muestra
    log_probs = np.zeros((n_samples,n_classes))

    # Paso 3: Iterar sobre cada muestra
    for i in range(n_samples):
        # Paso 3.1: Obtener la muestra actual
        x = X[i]

        # Paso 3.2: Iterar sobre cada clase
        for c in range(n_classes):
            # Paso 3.2.1: Calcular el logaritmo de la probabilidad a priori de la clase c
            log_prior = np.log(priors_estimates[c])

            # Paso 3.2.2: Calcular el logaritmo de las probabilidades condicionales de los píxeles dado la clase c
            log_likelihoods = x * np.log(thetas_estimates[:,c]) + (1-x) * np.log(1 - thetas_estimates[:, c] )

            # Paso 3.2.3: Sumar el logaritmo de las probabilidades condicionales de los píxeles
            log_likelihood_sum = np.sum(log_likelihoods)

            # Paso 3.2.4: Sumar el logaritmo de la probabilidad a priori y el logaritmo de las probabilidades condicionales
            log_prob = log_prior + log_likelihood_sum

            # Paso 3.2.5: Almacenar la probabilidad logarítmica de la clase c para la muestra actual
            log_probs[i,c] = log_prob

    # Paso 4: Encontrar la clase con la probabilidad logarítmica máxima para cada muestra
    predicted_classes = np.argmax(log_probs,axis=1)

    return predicted_classes

# Entrenar el modelo
priors_estimates, thetas_estimates = fit_naive_bayes(X_train, y_train)

# Predecir en el conjunto de prueba
y_pred = predict_naive_bayes(X_test, priors_estimates, thetas_estimates)

# Calcular y mostrar la precisión
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

"""# Vectores medios y generación de nuevos dígitos

# Vectores medios y generación de nuevos dígitos
"""

import numpy as np
import matplotlib.pyplot as plt

# Supongamos que 'thetas_estimates' es tu array de thetas de tamaño (10, 784),
# donde cada fila corresponde a una clase y cada columna a un píxel.
# Por ahora, lo dejaré como un placeholder. Debes reemplazarlo con tu propio array.
# thetas_estimates = tu_array_de_thetas

# Configuración para visualización: 10 dígitos (clases) y tamaño de imagen 28x28.
n_classes = 10
img_shape = (28,28)

# Crear una figura con subplots en una cuadrícula de 2x5
fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))

# Aplanar la matriz de ejes para facilitar su manejo en bucles
axes = axes.flatten()

for i in range(n_classes):
    # Extraer los parámetros theta de la clase i y redimensionarlos a la forma de la imagen.
    theta_img = (thetas_estimates*255)[:,i].reshape(img_shape)

    # Visualizar la imagen de theta: más brillante indica mayor probabilidad.
    axes[i].imshow(theta_img, cmap='gray', interpolation='nearest')

    # Quitar los ejes para una mejor visualización. # Importante la visualizacion no le importa al profe
    axes[i].axis('off')

    # Asignar un título a cada subplot con el dígito correspondiente.
    axes[i].set_title(f'Dígito: {i}')

# Ajustar espaciado entre plots y mostrar la figura completa
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Supongamos que 'thetas_estimates' es tu array de thetas de tamaño (784,10),
# donde cada fila corresponde a una clase y cada columna a un píxel.
# Por ahora, lo dejaré como un placeholder. Debes reemplazarlo con tu propio array.
# thetas_estimates = tu_array_de_thetas

# Configuración para visualización: 10 dígitos (clases) y tamaño de imagen 28x28.
n_classes = 10
img_shape = (28, 28)

# Crear una figura con subplots en una cuadrícula de 2x5
fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))

# Aplanar la matriz de ejes para facilitar su manejo en bucles
axes = axes.flatten()



for i in range(n_classes):
    # Extraer los parámetros theta de la clase i
    theta_class = thetas_estimates[:,i]

    # Tomamos una muestra para cada píxel de acuerdo a su valor de theta
    sampled_image = np.random.binomial(n=1, p=theta_class)

    # Redimensionamos la imagen a formato 2D
    sampled_image = sampled_image.reshape(28,28)

    # Visualizar la imagen de theta: más brillante indica mayor probabilidad.
    axes[i].imshow(sampled_image, cmap='gray', interpolation='nearest')

    # Quitar los ejes para una mejor visualización.
    axes[i].axis('off')

    # Asignar un título a cada subplot con el dígito correspondiente.
    axes[i].set_title(f'Dígito: {i}')

# Ajustar espaciado entre plots y mostrar la figura completa
plt.tight_layout()
plt.show()

n_classes = 10  # Número de clases, para MNIST esto es 10 (dígitos del 0 al 9)
n_features = 784  # Número de características (píxeles) para MNIST, que es 28x28

# Suponiendo que 'thetas_estimates' es tu array de thetas de tamaño (10, 784)
# Inicializa una matriz vacía para las muestras generadas
sampled_images = np.zeros((n_classes, n_features))

for i in range(n_classes):
    # Extraer los parámetros theta de la clase i
    theta_class = thetas_estimates[:,i]

    # Generar una muestra para cada píxel usando una distribución de Bernoulli
    # Nota: np.random.binomial realiza una "tirada" de una distribución de Bernoulli
    # donde 'n=1' (una sola prueba por píxel) y 'p' es la probabilidad (theta) para ese píxel
    sampled_image = np.random.binomial(n=1, p=theta_class)

    # Almacenar la imagen generada en nuestra matriz de imágenes muestreadas
    sampled_images[i] = sampled_image

# Visualización
fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(15, 6))
axes = axes.flatten()  # Aplanar la matriz de ejes para facilitar el manejo en bucles
for i, ax in enumerate(axes):
    # Mostrar la imagen muestreada para la clase i
    ax.imshow(sampled_images[i].reshape(28, 28), cmap='gray')
    ax.axis('off')  # Quitar los ejes
    ax.set_title(f'Sampled Digit: {i}')  # Establecer el título para cada subplot

plt.tight_layout()
plt.show()